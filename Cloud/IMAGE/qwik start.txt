You can list the active account name with this command:
>>>gcloud auth list
You can list the project ID with this command:
>>>gcloud config list project
Run the following command to install TensorFlow:
>>>pip install --user --upgrade tensorflow
Verify the installation:




Clone the example repo
In Cloud Shell run the following command to clone the cloudml-samples repo:


>>>git clone https://github.com/GoogleCloudPlatform/cloudml-samples.git

>>>cd cloudml-samples/census/estimator




Get your training data
The relevant data files, adult.data and adult.test, are hosted in a public Google Cloud Storage bucket.


Run the following command to download the data to a local file directory and set variables that point to the downloaded data files:


>>>mkdir data
>>>gsutil -m cp gs://cloud-samples-data/ml-engine/census/data/* data/

Now set the TRAIN_DATA and EVAL_DATA variables to your local file paths by running the following commands:

>>>export TRAIN_DATA=$(pwd)/data/adult.data.csv
>>>export EVAL_DATA=$(pwd)/data/adult.test.csv

To open the adult.data.csv file, run the following command:



>>>head data/adult.data.csv

>>>Install dependencies

>>>pip install --user -r ../requirements.txt


Run a local training job

Specify an output directory and set a MODEL_DIR variable by running the following command:

>>>export MODEL_DIR=output

Run this training job locally by running the following command:

>>>gcloud ai-platform local train \
    --module-name trainer.task \
    --package-path trainer/ \
    --job-dir $MODEL_DIR \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --eval-steps 100

Inspect the summary logs using Tensorboard

 >>>tensorboard --logdir=$MODEL_DIR --port=8080

Running model prediction locally (in Cloud Shell)

 >>>ls output/export/census/

 Copy the timestamp that was generated.


Run the following command in Cloud Shell, replacing <timestamp> with your timestamp:


 >>>gcloud ai-platform local predict \
--model-dir output/export/census/<timestamp> \
--json-instances ../test.json

You should see a result that looks something like the following:

CLASS_IDS  CLASSES  LOGISTIC              LOGITS                PROBABILITIES
[0]        [u'0']   [0.0583675391972065]  [-2.780855178833008]  [0.9416324496269226, 0.0583675354719162]

Where class 0 means income <= 50k and class 1 means income >50k.


The Cloud ML Engine services need to access Google Cloud Storage (GCS) to read and write data during model training and batch prediction.

First, set the following variables:

>>>PROJECT_ID=$(gcloud config list project --format "value(core.project)")
BUCKET_NAME=${PROJECT_ID}-mlengine
echo $BUCKET_NAME
REGION=us-central1

Create the new bucket:


>>>gsutil mb -l $REGION gs://$BUCKET_NAME

Use gsutil to copy the two files to your Cloud Storage bucket:

>>>gsutil cp -r data gs://$BUCKET_NAME/data

Set the TRAIN_DATA and EVAL_DATA variables to point to the files:

>>>TRAIN_DATA=gs://$BUCKET_NAME/data/adult.data.csv
>>>EVAL_DATA=gs://$BUCKET_NAME/data/adult.test.csv

Use gsutil again to copy the JSON test file test.json to your Cloud Storage bucket:

>>>gsutil cp ../test.json gs://$BUCKET_NAME/data/test.json

Set the TEST_JSON variable to point to that file:

>>>TEST_JSON=gs://$BUCKET_NAME/data/test.json

Run a single-instance trainer in the cloud
With a validated training job that runs in both single-instance and distributed mode, you're now ready to run a training job in the cloud. You'll start by requesting a single-instance training job.

Use the default BASIC scale tier to run a single-instance training job. The initial job request can take a few minutes to start, but subsequent jobs run more quickly. This enables quick iteration as you develop and validate your training job.

Select a name for the initial training run that distinguishes it from any subsequent training runs. For example, you can append a number to represent the iteration:

>>>JOB_NAME=census_single_1


Specify a directory for output generated by Cloud ML Engine by setting an OUTPUT_PATH variable to include when requesting training and prediction jobs. The OUTPUT_PATH represents the fully qualified Cloud Storage location for model checkpoints, summaries, and exports. You can use the BUCKET_NAME variable you defined in a previous step. It's a good practice to use the job name as the output directory:

>>>OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME

Run the following command to submit a training job in the cloud that uses a single process. This time, set the --verbosity tag to DEBUG so that you can inspect the full logging output and retrieve accuracy, loss, and other metrics. The output also contains a number of other warning messages that you can ignore for the purposes of this sample:


>>>gcloud ai-platform jobs submit training $JOB_NAME \
    --job-dir $OUTPUT_PATH \
    --runtime-version 1.10 \
    --module-name trainer.task \
    --package-path trainer/ \
    --region $REGION \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --eval-steps 100 \
    --verbosity DEBUG


You can monitor the progress of your training job by watching the logs on the command line:


>>> gcloud ai-platform jobs stream-logs $JOB_NAME


In cloud training, outputs are produced in Cloud Storage. In this sample, outputs are saved to OUTPUT_PATH; to list them, run:


 >>>gsutil ls -r $OUTPUT_PATH


The outputs should be similar to the outputs from training locally (above).

Optional: You can point TensorBoard to this directory, either during or after training using the Web Preview menu at the top of the Cloud Shell and again select Preview on port 8080:

 >>>tensorboard --logdir=$OUTPUT_PATH --port=8080


Deploy your model to support prediction
By deploying your trained model to Cloud ML Engine to serve online prediction requests, you get the benefit of scalable serving. This is useful if you expect your trained model to be hit with many prediction requests in a short period of time.

Wait until your CMLE training job is done. It is finished when you see a green check mark by the jobname in the Cloud Console, or when you see the message Job completed successfully in the Cloud Shell command line.

Create a Cloud ML Engine model:


 >>>MODEL_NAME=census

Create a Cloud ML Engine model:

 >>>gcloud ai-platform models create $MODEL_NAME --regions=$REGION

Select the exported model to use, by looking up the full path of your exported trained model binaries.


 >>>gsutil ls -r $OUTPUT_PATH/export

Scroll through the output to find the value of $OUTPUT_PATH/export/census/<timestamp>/. Copy timestamp and add it to the following command to set the environment variable MODEL_BINARIES to its value:

 >>>MODEL_BINARIES=$OUTPUT_PATH/export/census/<timestamp>/
 You'll deploy this trained model.

Run the following command to create a version v1 of your model:



 >>>gcloud ai-platform versions create v1 \
--model $MODEL_NAME \
--origin $MODEL_BINARIES \
--runtime-version 1.10


It may take several minutes to deploy your trained model. When done, you can see a list of your models using the models list command:

gcloud ai-platform models list

Send an online prediction request to your deployed model
You can now send prediction requests to your deployed model. The following command sends a prediction request using the test.json file included in the GitHub repository.


>>>gcloud ai-platform predict \
--model $MODEL_NAME \
--version v1 \
--json-instances ../test.json

The response includes the probabilities of each label (>50K and <=50K) based on the data entry in test.json, thus indicating whether the predicted income is greater than or less than 50,000 dollars

CLASS_IDS  CLASSES  LOGISTIC               LOGITS                PROBABILITIES
[0]        [u'0']   [0.06142739951610565]  [-2.726504325866699]  [0.9385725855827332, 0.061427392065525055]
Where class 0 means income <= 50k and class 1 means income >50k.